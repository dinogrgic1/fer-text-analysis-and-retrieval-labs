{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77230ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ed8a00df369ce38f9743ed412973d72b",
     "grade": false,
     "grade_id": "cell-cbdf7d65ea58446b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "University of Zagreb\\\n",
    "Faculty of Electrical Engineering and Computing\n",
    "\n",
    "## Text Analysis and Retrieval 2021/2022\n",
    "https://www.fer.unizg.hr/predmet/apt/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6879a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dee22e58",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a7a9536ec842755316f7ccaa8971ac0",
     "grade": false,
     "grade_id": "cell-5e9c1e104dec0dd4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------------------------------\n",
    "\n",
    "### Neural NLP\n",
    "### LAB 3\n",
    "\n",
    "\n",
    "*Version: 1.0*\n",
    "\n",
    "(c) 2022 Josip Jukić, Jan Šnajder\n",
    "\n",
    "Submission deadline: **May 29, 2022, 23:59 CET** \n",
    "\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6333437b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6233aca5abb9b3f8b6a7a6080fb096fb",
     "grade": false,
     "grade_id": "cell-b25d76fa7c847af2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "Hello visitor, this lab assignment consists of three parts. Your task boils down to filling out the missing parts of code and evaluating the cells. These parts are indicated by the \"YOUR CODE HERE\" template.\n",
    "\n",
    "Each subtask is supplemented by several tests that you can run. Apart from that, there are additional test that will be executed after submition. If your solution is valid and it passes all of the visible tests, there shouldn't be any problems with the additional tests.\n",
    "\n",
    "**IMPORTANT: Don't change the names of the predefined methods or random seeds**, because the tests won't execute properly.\n",
    "\n",
    "You're required to do this assignment **on your own**.\n",
    "\n",
    "If you stumble upon problems, please refer to josip.jukic@fer.hr for office hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2336f31b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd1654b2c7c8de8e1d1cfaeeee261ec2",
     "grade": false,
     "grade_id": "cell-150c23ae802b9522",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52503faa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cf13460796faed6f774e768345186e9",
     "grade": false,
     "grade_id": "cell-95afad8333fec3bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1. Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e247b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "563e4d2c420bc52c4d04db999317a738",
     "grade": false,
     "grade_id": "cell-7caecd650447b599",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Machine Translation (MT) is the task of automatically converting one natural language into another, preserving the meaning of the input text, and producing fluent text in the output language. While machine translation is one of the oldest subfields of artificial intelligence research, the recent shift towards large-scale empirical techniques has led to very significant improvements in translation quality.\n",
    "\n",
    "In this lab assignment, we will use pre-trained sequence-to-sequence models for machine translation. Additionally, we will consider two text generation strategies: greedy decoder and beam search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673b2d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7a655abf3ffcbc41eae639cf60ac9c8",
     "grade": false,
     "grade_id": "cell-9c4e4d599e282ec6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### (a)\n",
    "\n",
    "Assuming that we are in possesion of a pre-trained model, we still need to figure out how exactly are we going to generate tokens. One of the simplest approaches is to retrieve the most probable token in each step.\n",
    "Implement `greedy_decoder` for language generation. Greedy method retrieves the index of the most probable token for each timestep in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80305ded",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "834924716c12bf23b38e72cd7c43d350",
     "grade": false,
     "grade_id": "cell-2bc8e6ac47722f0f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def greedy_decoder(array):\n",
    "    return np.argmax(array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb28c515",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e800457ec180902671743277b02cd02",
     "grade": true,
     "grade_id": "cell-e6919b45595c671e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ex1a1 = np.array(\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.5, 0.4],\n",
    "        [0.2, 0.1, 0.3, 0.4, 0.5],\n",
    "        [0.3, 0.4, 0.5, 0.2, 0.1],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.5, 0.3, 0.4, 0.2],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "sol1a1 = np.array([4, 0, 4, 0, 3, 4, 2, 0, 1, 0])\n",
    "\n",
    "assert (greedy_decoder(ex1a1) == sol1a1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca894a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1ae2ab666f197db3ecff1e7b440f622",
     "grade": false,
     "grade_id": "cell-37e0ec725ec6b044",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### (b)\n",
    "\n",
    "Greedy approach has the benefit that it is very fast, but the quality of the final output sequences may be far from optimal. Instead of greedily choosing the most likely next step as the sequence is constructed, the beam search expands all possible next steps and keeps the `k` most likely, where `k` is a parameter and controls the number of beams or parallel searches through the sequence of probabilities.\n",
    "\n",
    "The local beam search algorithm keeps track of `k` states rather than just one. It begins with `k` randomly generated states. At each step, all the successors of all `k` states are generated. If any one is a goal, the algorithm halts. Otherwise, it selects the k best successors from the complete list and repeats. This is illustrated in the figure below.\n",
    "\n",
    "Implement `beam_search_decoder`.\n",
    "\n",
    "![Beam search](img/beamsearch.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6f35d42",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6035a93ffd07d26b14e90ca24b418212",
     "grade": false,
     "grade_id": "cell-1daf6938107bcf6e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def beam_search_decoder(data, k):\n",
    "    sequences = [[list(), 0.0]]\n",
    "    \n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            for j in range(len(row)):\n",
    "                candidate = [seq + [j], score - np.log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        print(all_candidates)\n",
    "                \n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "        sequences = ordered[:k]\n",
    "    print(sequences)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "077146a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fff4a789b596cc095da7a895d1e83bd",
     "grade": true,
     "grade_id": "cell-2752c4f710125130",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0], 2.3025850929940455], [[1], 1.6094379124341003], [[2], 1.2039728043259361], [[3], 0.916290731874155], [[4], 0.6931471805599453]]\n",
      "[[[4, 0], 1.3862943611198906], [[4, 1], 1.6094379124341003], [[4, 2], 1.8971199848858813], [[4, 3], 2.3025850929940455], [[4, 4], 2.995732273553991], [[3, 0], 1.6094379124341003], [[3, 1], 1.83258146374831], [[3, 2], 2.120263536200091], [[3, 3], 2.525728644308255], [[3, 4], 3.2188758248682006], [[2, 0], 1.8971199848858813], [[2, 1], 2.120263536200091], [[2, 2], 2.4079456086518722], [[2, 3], 2.8134107167600364], [[2, 4], 3.506557897319982]]\n",
      "[[[4, 0, 0], 3.688879454113936], [[4, 0, 1], 2.995732273553991], [[4, 0, 2], 2.5902671654458267], [[4, 0, 3], 2.3025850929940455], [[4, 0, 4], 2.0794415416798357], [[4, 1, 0], 3.9120230054281455], [[4, 1, 1], 3.2188758248682006], [[4, 1, 2], 2.8134107167600364], [[4, 1, 3], 2.525728644308255], [[4, 1, 4], 2.3025850929940455], [[3, 0, 0], 3.9120230054281455], [[3, 0, 1], 3.2188758248682006], [[3, 0, 2], 2.8134107167600364], [[3, 0, 3], 2.525728644308255], [[3, 0, 4], 2.3025850929940455]]\n",
      "[[[4, 0, 4, 0], 2.772588722239781], [[4, 0, 4, 1], 2.995732273553991], [[4, 0, 4, 2], 3.283414346005772], [[4, 0, 4, 3], 3.688879454113936], [[4, 0, 4, 4], 4.382026634673881], [[4, 0, 3, 0], 2.995732273553991], [[4, 0, 3, 1], 3.2188758248682006], [[4, 0, 3, 2], 3.506557897319982], [[4, 0, 3, 3], 3.9120230054281455], [[4, 0, 3, 4], 4.605170185988091], [[4, 1, 4, 0], 2.995732273553991], [[4, 1, 4, 1], 3.2188758248682006], [[4, 1, 4, 2], 3.506557897319982], [[4, 1, 4, 3], 3.9120230054281455], [[4, 1, 4, 4], 4.605170185988091]]\n",
      "[[[4, 0, 4, 0, 0], 5.075173815233827], [[4, 0, 4, 0, 1], 4.382026634673881], [[4, 0, 4, 0, 2], 3.9765615265657175], [[4, 0, 4, 0, 3], 3.4657359027997265], [[4, 0, 4, 0, 4], 3.6888794541139363], [[4, 0, 4, 1, 0], 5.298317366548036], [[4, 0, 4, 1, 1], 4.605170185988091], [[4, 0, 4, 1, 2], 4.199705077879927], [[4, 0, 4, 1, 3], 3.6888794541139363], [[4, 0, 4, 1, 4], 3.912023005428146], [[4, 0, 3, 0, 0], 5.298317366548036], [[4, 0, 3, 0, 1], 4.605170185988091], [[4, 0, 3, 0, 2], 4.199705077879927], [[4, 0, 3, 0, 3], 3.6888794541139363], [[4, 0, 3, 0, 4], 3.912023005428146]]\n",
      "[[[4, 0, 4, 0, 3, 0], 5.075173815233827], [[4, 0, 4, 0, 3, 1], 5.768320995793772], [[4, 0, 4, 0, 3, 2], 4.669708707125663], [[4, 0, 4, 0, 3, 3], 4.382026634673881], [[4, 0, 4, 0, 3, 4], 4.1588830833596715], [[4, 0, 4, 0, 4, 0], 5.298317366548036], [[4, 0, 4, 0, 4, 1], 5.991464547107982], [[4, 0, 4, 0, 4, 2], 4.892852258439873], [[4, 0, 4, 0, 4, 3], 4.605170185988091], [[4, 0, 4, 0, 4, 4], 4.382026634673881], [[4, 0, 4, 1, 3, 0], 5.298317366548036], [[4, 0, 4, 1, 3, 1], 5.991464547107982], [[4, 0, 4, 1, 3, 2], 4.892852258439873], [[4, 0, 4, 1, 3, 3], 4.605170185988091], [[4, 0, 4, 1, 3, 4], 4.382026634673881]]\n",
      "[[[4, 0, 4, 0, 3, 4, 0], 5.362855887685607], [[4, 0, 4, 0, 3, 4, 1], 5.075173815233827], [[4, 0, 4, 0, 3, 4, 2], 4.852030263919617], [[4, 0, 4, 0, 3, 4, 3], 5.768320995793772], [[4, 0, 4, 0, 3, 4, 4], 6.461468176353717], [[4, 0, 4, 0, 3, 3, 0], 5.585999438999817], [[4, 0, 4, 0, 3, 3, 1], 5.298317366548036], [[4, 0, 4, 0, 3, 3, 2], 5.075173815233827], [[4, 0, 4, 0, 3, 3, 3], 5.991464547107982], [[4, 0, 4, 0, 3, 3, 4], 6.684611727667926], [[4, 0, 4, 0, 4, 4, 0], 5.585999438999817], [[4, 0, 4, 0, 4, 4, 1], 5.298317366548036], [[4, 0, 4, 0, 4, 4, 2], 5.075173815233827], [[4, 0, 4, 0, 4, 4, 3], 5.991464547107982], [[4, 0, 4, 0, 4, 4, 4], 6.684611727667926]]\n",
      "[[[4, 0, 4, 0, 3, 4, 2, 0], 5.545177444479562], [[4, 0, 4, 0, 3, 4, 2, 1], 5.768320995793772], [[4, 0, 4, 0, 3, 4, 2, 2], 6.056003068245553], [[4, 0, 4, 0, 3, 4, 2, 3], 6.461468176353717], [[4, 0, 4, 0, 3, 4, 2, 4], 7.154615356913663], [[4, 0, 4, 0, 3, 4, 1, 0], 5.768320995793772], [[4, 0, 4, 0, 3, 4, 1, 1], 5.991464547107982], [[4, 0, 4, 0, 3, 4, 1, 2], 6.2791466195597625], [[4, 0, 4, 0, 3, 4, 1, 3], 6.684611727667927], [[4, 0, 4, 0, 3, 4, 1, 4], 7.377758908227872], [[4, 0, 4, 0, 3, 3, 2, 0], 5.768320995793772], [[4, 0, 4, 0, 3, 3, 2, 1], 5.991464547107982], [[4, 0, 4, 0, 3, 3, 2, 2], 6.2791466195597625], [[4, 0, 4, 0, 3, 3, 2, 3], 6.684611727667927], [[4, 0, 4, 0, 3, 3, 2, 4], 7.377758908227872]]\n",
      "[[[4, 0, 4, 0, 3, 4, 2, 0, 0], 7.847762537473608], [[4, 0, 4, 0, 3, 4, 2, 0, 1], 6.238324625039508], [[4, 0, 4, 0, 3, 4, 2, 0, 2], 6.749150248805498], [[4, 0, 4, 0, 3, 4, 2, 0, 3], 6.461468176353717], [[4, 0, 4, 0, 3, 4, 2, 0, 4], 7.154615356913663], [[4, 0, 4, 0, 3, 4, 2, 1, 0], 8.070906088787817], [[4, 0, 4, 0, 3, 4, 2, 1, 1], 6.461468176353717], [[4, 0, 4, 0, 3, 4, 2, 1, 2], 6.972293800119708], [[4, 0, 4, 0, 3, 4, 2, 1, 3], 6.684611727667927], [[4, 0, 4, 0, 3, 4, 2, 1, 4], 7.3777589082278725], [[4, 0, 4, 0, 3, 4, 1, 0, 0], 8.070906088787817], [[4, 0, 4, 0, 3, 4, 1, 0, 1], 6.461468176353717], [[4, 0, 4, 0, 3, 4, 1, 0, 2], 6.972293800119708], [[4, 0, 4, 0, 3, 4, 1, 0, 3], 6.684611727667927], [[4, 0, 4, 0, 3, 4, 1, 0, 4], 7.3777589082278725]]\n",
      "[[[4, 0, 4, 0, 3, 4, 2, 0, 1, 0], 6.931471805599453], [[4, 0, 4, 0, 3, 4, 2, 0, 1, 1], 7.154615356913663], [[4, 0, 4, 0, 3, 4, 2, 0, 1, 2], 7.442297429365444], [[4, 0, 4, 0, 3, 4, 2, 0, 1, 3], 7.847762537473608], [[4, 0, 4, 0, 3, 4, 2, 0, 1, 4], 8.540909718033554], [[4, 0, 4, 0, 3, 4, 2, 0, 3, 0], 7.154615356913663], [[4, 0, 4, 0, 3, 4, 2, 0, 3, 1], 7.3777589082278725], [[4, 0, 4, 0, 3, 4, 2, 0, 3, 2], 7.665440980679653], [[4, 0, 4, 0, 3, 4, 2, 0, 3, 3], 8.070906088787817], [[4, 0, 4, 0, 3, 4, 2, 0, 3, 4], 8.764053269347762], [[4, 0, 4, 0, 3, 4, 2, 1, 1, 0], 7.154615356913663], [[4, 0, 4, 0, 3, 4, 2, 1, 1, 1], 7.3777589082278725], [[4, 0, 4, 0, 3, 4, 2, 1, 1, 2], 7.665440980679653], [[4, 0, 4, 0, 3, 4, 2, 1, 1, 3], 8.070906088787817], [[4, 0, 4, 0, 3, 4, 2, 1, 1, 4], 8.764053269347762]]\n",
      "[[[4, 0, 4, 0, 3, 4, 2, 0, 1, 0], 6.931471805599453], [[4, 0, 4, 0, 3, 4, 2, 0, 1, 1], 7.154615356913663], [[4, 0, 4, 0, 3, 4, 2, 0, 3, 0], 7.154615356913663]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinog\\AppData\\Local\\Temp/ipykernel_21832/2076933134.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  assert (beam_search_decoder(ex1b1, 3) == sol1b1).all()\n",
      "C:\\Users\\dinog\\AppData\\Local\\Temp/ipykernel_21832/2076933134.py:24: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  assert (beam_search_decoder(ex1b1, 3) == sol1b1).all()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21832/2076933134.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbeam_search_decoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex1b1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msol1b1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'all'"
     ]
    }
   ],
   "source": [
    "ex1b1 = np.array(\n",
    "    [\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.2, 0.3, 0.5, 0.4],\n",
    "        [0.2, 0.1, 0.3, 0.4, 0.5],\n",
    "        [0.3, 0.4, 0.5, 0.2, 0.1],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "        [0.1, 0.5, 0.3, 0.4, 0.2],\n",
    "        [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "    ]\n",
    ")\n",
    "\n",
    "sol1b1 = np.array(\n",
    "    [\n",
    "        [4, 0, 4, 0, 3, 4, 2, 0, 1, 0],\n",
    "        [4, 0, 4, 0, 3, 4, 2, 0, 1, 1],\n",
    "        [4, 0, 4, 0, 3, 4, 2, 0, 1, 2],\n",
    "    ]\n",
    ")\n",
    "\n",
    "assert (beam_search_decoder(ex1b1, 3) == sol1b1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5026aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b0cb3ec2ceac3f620300ee39d7086d92",
     "grade": false,
     "grade_id": "cell-be562f4995d3584e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### (c)\n",
    "\n",
    "Finally, let's employ a pre-trained sequence-to-sequence transformer from the [`hugggingface`](https://huggingface.co/) library. Specifically, we are going to use `mBART-large-50`, which can be employed in the [zero-shot](https://en.wikipedia.org/wiki/Zero-shot_learning) setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85def3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "# Load the model and its corresponding tokenizer.\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57288b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07ad58e558933499e4b1b5e40188d73a",
     "grade": false,
     "grade_id": "cell-7247da3628b8c124",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Take a look at the example below to see how to use the pre-trained model in the zero-shot setup. We are translating from Finnish (fi_FI) to English (en_XX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc099eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The morning goes by in bed, and the day ends in reverse.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi_text = \"Aamu kuluu aatellessa, päivä päätä käännellessä.\"\n",
    "tokenizer.src_lang = \"fi_FI\"\n",
    "encoded_ar = tokenizer(fi_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(\n",
    "    **encoded_ar, max_length=50, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
    ")\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf1191",
   "metadata": {},
   "source": [
    "We can control the decoding algorithm by setting the `num_beams` parameter. If it is set to `None`, tokens will be decoded greedily. Additionally, we can control the sequence max length with `max_length` and early stopping with `early_stopping`. When set to True, `early_stopping` indicates that generation is finished when all beam hypotheses reached the EOS (end-of-sequence) token.\n",
    "\n",
    "Implement the `translate` method which generalizes translation using `mbart` to any source and target language supported by the model. If in doubt, refer to the previous example. Be sure to use all of the arguments, most of which are going to be forwarded to the model's `generate` method. `batch_decode` wraps the decoded tokens into a list, so don't forget to extract the first element from the list to get the actual string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12e4c8d0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ba3a8ae65344a27cf772c43ca00b2f6",
     "grade": false,
     "grade_id": "cell-30758986b31cf01b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def translate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text,\n",
    "    src_lang,\n",
    "    tgt_lang=\"en_XX\",\n",
    "    num_beams=None,\n",
    "    max_length=50,\n",
    "    early_stopping=True,\n",
    "):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    encoded_ar = tokenizer(text, return_tensors=\"pt\")\n",
    "    generated_tokens = model.generate(**encoded_ar, max_length=max_length, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])\n",
    "    return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b102b",
   "metadata": {},
   "source": [
    "Translate Finnish (fi_FI) and Portugese (pt_PT) texts to English (see the cell below) using the `translate` method. Is greedy decoding doing OK or is it better to use beam search? If beam search is better, which `k` seems to perform well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "155e2861",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_text_1 = \"Ahneus vie linnut taivaalta ja kalat merestä.\"\n",
    "fi_text_2 = \"Aika on rahaa, sanoi työtön kun kellonsa myi.\"\n",
    "\n",
    "pt_text_1 = \"Mais vale um pássaro na mão do que dois voando.\"\n",
    "pt_text_2 = \"Diz-me com quem andas e eu te direi quem és.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e7fe29b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6fc0d5e39579a4429678e4c7312109f",
     "grade": true,
     "grade_id": "cell-00bb6ec8ed77cc00",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_beans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21832/3374132771.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mex1c1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Anfangen ist leicht, Beharren ist Kunst\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex1c1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"de_DE\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Starting is easy, barking is art\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21832/266627641.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(model, tokenizer, text, src_lang, tgt_lang, num_beams, max_length, early_stopping)\u001b[0m\n\u001b[0;32m     10\u001b[0m ):\n\u001b[0;32m     11\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_lang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mencoded_ar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beans\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_beans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mgenerated_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mencoded_ar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforced_bos_token_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang_code_to_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtgt_lang\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_beans' is not defined"
     ]
    }
   ],
   "source": [
    "ex1c1 = \"Anfangen ist leicht, Beharren ist Kunst\"\n",
    "assert translate(model, tokenizer, ex1c1, \"de_DE\") == \"Starting is easy, barking is art\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20000a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
